{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7abc88f-53f8-4258-b343-0866d68a8482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6896ffc-24ac-4285-b372-69983e2360d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEASON_ID</th>\n",
       "      <th>TEAM_ABBREVIATION</th>\n",
       "      <th>PLAYER_AGE</th>\n",
       "      <th>GP</th>\n",
       "      <th>GS</th>\n",
       "      <th>MIN</th>\n",
       "      <th>FGM</th>\n",
       "      <th>FGA</th>\n",
       "      <th>FG_PCT</th>\n",
       "      <th>FG3M</th>\n",
       "      <th>...</th>\n",
       "      <th>FTA_PER_GAME</th>\n",
       "      <th>OREB_PER_GAME</th>\n",
       "      <th>DREB_PER_GAME</th>\n",
       "      <th>REB_PER_GAME</th>\n",
       "      <th>AST_PER_GAME</th>\n",
       "      <th>STL_PER_GAME</th>\n",
       "      <th>BLK_PER_GAME</th>\n",
       "      <th>TOV_PER_GAME</th>\n",
       "      <th>PF_PER_GAME</th>\n",
       "      <th>PTS_PER_GAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1985-86</td>\n",
       "      <td>LAL</td>\n",
       "      <td>22.0</td>\n",
       "      <td>82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1542.0</td>\n",
       "      <td>209</td>\n",
       "      <td>388</td>\n",
       "      <td>0.539</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.04</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.70</td>\n",
       "      <td>4.65</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.21</td>\n",
       "      <td>2.79</td>\n",
       "      <td>6.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1986-87</td>\n",
       "      <td>LAL</td>\n",
       "      <td>23.0</td>\n",
       "      <td>79</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2240.0</td>\n",
       "      <td>316</td>\n",
       "      <td>587</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.57</td>\n",
       "      <td>2.66</td>\n",
       "      <td>5.13</td>\n",
       "      <td>7.78</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.29</td>\n",
       "      <td>2.16</td>\n",
       "      <td>10.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1987-88</td>\n",
       "      <td>LAL</td>\n",
       "      <td>24.0</td>\n",
       "      <td>82</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2636.0</td>\n",
       "      <td>322</td>\n",
       "      <td>640</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.62</td>\n",
       "      <td>2.99</td>\n",
       "      <td>5.67</td>\n",
       "      <td>8.66</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.46</td>\n",
       "      <td>2.49</td>\n",
       "      <td>11.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1988-89</td>\n",
       "      <td>LAL</td>\n",
       "      <td>25.0</td>\n",
       "      <td>82</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2510.0</td>\n",
       "      <td>401</td>\n",
       "      <td>758</td>\n",
       "      <td>0.529</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.38</td>\n",
       "      <td>3.15</td>\n",
       "      <td>5.87</td>\n",
       "      <td>9.01</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.10</td>\n",
       "      <td>13.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1989-90</td>\n",
       "      <td>LAL</td>\n",
       "      <td>26.0</td>\n",
       "      <td>82</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2709.0</td>\n",
       "      <td>385</td>\n",
       "      <td>806</td>\n",
       "      <td>0.478</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.51</td>\n",
       "      <td>3.20</td>\n",
       "      <td>5.49</td>\n",
       "      <td>8.68</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.41</td>\n",
       "      <td>2.52</td>\n",
       "      <td>12.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  SEASON_ID TEAM_ABBREVIATION  PLAYER_AGE  GP    GS     MIN  FGM  FGA  FG_PCT  \\\n",
       "0   1985-86               LAL        22.0  82   1.0  1542.0  209  388   0.539   \n",
       "1   1986-87               LAL        23.0  79  72.0  2240.0  316  587   0.538   \n",
       "2   1987-88               LAL        24.0  82  64.0  2636.0  322  640   0.503   \n",
       "3   1988-89               LAL        25.0  82  82.0  2510.0  401  758   0.529   \n",
       "4   1989-90               LAL        26.0  82  82.0  2709.0  385  806   0.478   \n",
       "\n",
       "   FG3M  ...  FTA_PER_GAME  OREB_PER_GAME  DREB_PER_GAME  REB_PER_GAME  \\\n",
       "0   1.0  ...          2.04           1.95           2.70          4.65   \n",
       "1   0.0  ...          3.57           2.66           5.13          7.78   \n",
       "2   0.0  ...          4.62           2.99           5.67          8.66   \n",
       "3   4.0  ...          4.38           3.15           5.87          9.01   \n",
       "4  13.0  ...          4.51           3.20           5.49          8.68   \n",
       "\n",
       "   AST_PER_GAME  STL_PER_GAME  BLK_PER_GAME  TOV_PER_GAME  PF_PER_GAME  \\\n",
       "0          0.66          0.60          0.60          1.21         2.79   \n",
       "1          1.06          0.89          1.01          1.29         2.16   \n",
       "2          1.13          1.06          0.55          1.46         2.49   \n",
       "3          1.26          1.15          0.67          1.45         2.10   \n",
       "4          1.10          0.80          0.61          1.41         2.52   \n",
       "\n",
       "   PTS_PER_GAME  \n",
       "0          6.35  \n",
       "1         10.78  \n",
       "2         11.43  \n",
       "3         13.27  \n",
       "4         12.94  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full = pd.read_csv(\"nba_player_stats_20250304.csv\")\n",
    "\n",
    "data_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c1d7f15-f021-462f-92a0-06de900a9b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs found in the data\n"
     ]
    }
   ],
   "source": [
    "def check_for_nans(data):\n",
    "    \"\"\"\n",
    "    Check for NaNs in the dataset.\n",
    "    \"\"\"\n",
    "    if data.isna().any().any():\n",
    "        print(\"NaNs found in the data\")\n",
    "    else:\n",
    "        print(\"No NaNs found in the data\")\n",
    "\n",
    "check_for_nans(data_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47270450-166b-41a7-aa7d-6fd31b9aba52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with NaNs: ['GS', 'FG_PCT', 'FG3_PCT', 'FT_PCT']\n"
     ]
    }
   ],
   "source": [
    "def print_rows_with_nans(df):\n",
    "    \"\"\"\n",
    "    Prints out all rows in the DataFrame that contain NaNs along with the unique columns.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to check for NaNs.\n",
    "    \"\"\"\n",
    "    rows_with_nans = df[df.isna().any(axis=1)]\n",
    "    nan_columns = rows_with_nans.columns[rows_with_nans.isna().any()].tolist()\n",
    "    print(f\"Columns with NaNs: {nan_columns}\")\n",
    "\n",
    "print_rows_with_nans(data_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0216bff-0be0-4d36-980a-6eb8a0514e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for players traded mid-season, the data has multiple rows containing information for each team they played for. \n",
    "# this breaks the year-by-year sequential structure i want.\n",
    "data_full[\"is_traded\"] = data_full.duplicated(subset=[\"SEASON_ID\", \"PLAYER_NAME\"], keep=False).astype(int) \n",
    "# creating a new column to represent seasons where the player was traded\n",
    "data_full = data_full[(data_full[\"TEAM_ABBREVIATION\"] == \"TOT\") | (data_full[\"is_traded\"] == 0)] # dropping all non \"TOT\" rows for traded players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69d348d0-fb94-4159-818f-3bbcb5e758f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = data_full.drop(columns=['GS', 'MIN', 'FGM', 'FGA', 'FG_PCT', 'FG3M', 'FG3A',\n",
    "       'FG3_PCT', 'FTM', 'FTA', 'FT_PCT', 'OREB', 'DREB', 'REB', 'AST', 'STL',\n",
    "       'BLK', 'TOV', 'PF', 'PTS']) # these are season totals, i need per game values\n",
    "\n",
    "# Since FG%, GS, FG3%, FT% have nan values I will drop them. They either have redundant data or meaningless data in the case of GS (IMO).\n",
    "# I will add a masking layer so that they don't interfere with the model.\n",
    "# A cleaner solution would be to drop the columns or remove old data but I'll do it this way since there's already so little data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55d369d7-57ce-48e3-819b-7dc2a0524a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = data_full.drop(columns=[\"TEAM_ABBREVIATION\", \"SEASON_ID\"]) # dropping the unnecessary columns except season_start_year and player_name\n",
    "# we'll need them for sorting and grouping later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bccec54-9ace-40e2-8ef3-0fe5e3004349",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = data_full.fillna(-999.0) # just in case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f72ad11-8d62-4232-9616-63d969397f2a",
   "metadata": {},
   "source": [
    "We need to normalize the data but we cant do it normally since there are masked values of -999.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93342d10-eead-4b92-ab6a-ba69992f7e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_full[data_full[\"SEASON_START_YEAR\"] != 2025] # since the 2024-2025 season isnt complete, it shouldnt be used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4173fbc0-13b9-4262-86f3-5501dda9fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normalization_params(df, mask_value=-999.0):\n",
    "    \"\"\"\n",
    "    Compute per-column means and stds ignoring the masked value.\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "    means = {}\n",
    "    stds = {}\n",
    "    for col in columns:\n",
    "        # need to filter out the masked values before computing the statistics\n",
    "        valid_data = df.loc[df[col] != mask_value, col]\n",
    "        means[col] = valid_data.mean()\n",
    "        stds[col] = valid_data.std()\n",
    "    return means, stds\n",
    "\n",
    "def normalize_data(df, means, stds, mask_value=-999.0):\n",
    "    \"\"\"\n",
    "    Normalize data column-wise using provided means and stds while leaving masked values unchanged.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_normalized = df.copy()\n",
    "    columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "    for col in columns:\n",
    "        df_normalized[col] = df_normalized[col].astype(\"float64\")\n",
    "        valid_mask = df_normalized[col] != mask_value # normalize only valid entries\n",
    "        df_normalized.loc[valid_mask, col] = (df_normalized.loc[valid_mask, col] - means[col]) / stds[col]\n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd570566-18c7-4a31-b429-58c61a674d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "means, stds = compute_normalization_params(data_full)\n",
    "\n",
    "data_full_normalized = normalize_data(data_full, means, stds, mask_value=-999.0)\n",
    "data_normalized = normalize_data(data, means, stds, mask_value=-999.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75b24111-65da-4f65-9936-3be59b2a4263",
   "metadata": {},
   "outputs": [],
   "source": [
    "players = data[\"PLAYER_NAME\"].unique()\n",
    "train_players, val_players = train_test_split(players, test_size=0.1, random_state=42) \n",
    "# i split before creating the sequences. if i split afterwards, i will not be able to validate on out of sample data.\n",
    "\n",
    "train_data = data_normalized[data_normalized[\"PLAYER_NAME\"].isin(train_players)]\n",
    "val_data = data_normalized[data_normalized[\"PLAYER_NAME\"].isin(val_players)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4d2b9a3-08cb-4392-adaf-31ee89c57942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PLAYER_AGE', 'GP', 'SEASON_START_YEAR', 'PLAYER_NAME', 'MIN_PER_GAME',\n",
       "       'FGM_PER_GAME', 'FGA_PER_GAME', 'FG3M_PER_GAME', 'FG3A_PER_GAME',\n",
       "       'FTM_PER_GAME', 'FTA_PER_GAME', 'OREB_PER_GAME', 'DREB_PER_GAME',\n",
       "       'REB_PER_GAME', 'AST_PER_GAME', 'STL_PER_GAME', 'BLK_PER_GAME',\n",
       "       'TOV_PER_GAME', 'PF_PER_GAME', 'PTS_PER_GAME', 'is_traded'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "483e3602-29e8-46da-b3aa-3b19bde2002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full.to_csv(\"data_full.csv\", index=False)\n",
    "data.to_csv(\"data.csv\", index=False) # to save preprocessed versions\n",
    "\n",
    "data_full_normalized.to_csv('data_full_normalized.csv', index=False)\n",
    "data_normalized.to_csv('data_normalized.csv', index=False) # have to create a separate file since the season year value is normalized as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42b17b8e-01f9-4918-a4b7-a0a9676c92b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data):\n",
    "    \"\"\"\n",
    "    Create sequences for the entire career of a player.\n",
    "    Each sequence grows in length and the label is the stats we're trying to predict for the following year.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    player_info = []\n",
    "    player_groups = data.groupby(\"PLAYER_NAME\")\n",
    "    \n",
    "    for player_name, group in player_groups:\n",
    "        group = group.sort_values(by=\"SEASON_START_YEAR\").reset_index(drop=True)\n",
    "        \n",
    "        for i in range(1, len(group)):\n",
    "            seq = group.iloc[:i].drop(columns=[\"PLAYER_NAME\", \"SEASON_START_YEAR\"]).values\n",
    "            label = group.iloc[[i]].drop(columns=[\"PLAYER_AGE\", \"PLAYER_NAME\", \"SEASON_START_YEAR\", \"is_traded\"]).values.flatten()\n",
    "            sequences.append(seq)\n",
    "            labels.append(label)\n",
    "            player_info.append((player_name, group.iloc[i][\"SEASON_START_YEAR\"])) \n",
    "            # need to retain info to know what and whose season we're predicting later on\n",
    "    \n",
    "    return sequences, labels, player_info\n",
    "    # the output needs to be flattened to go back to 1d label structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a150a84c-e1d7-40f0-afc7-01d7d8deebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences, train_labels, train_player_info = create_sequences(train_data)\n",
    "val_sequences, val_labels, val_player_info = create_sequences(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53364812-681a-4ca5-8e4d-8a9ec9ccc29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, dtype=\"float32\", padding=\"pre\", value = -999.0)\n",
    "val_sequences = tf.keras.preprocessing.sequence.pad_sequences(val_sequences, dtype=\"float32\", padding=\"pre\", value = -999.0)\n",
    "\n",
    "# i'm padding the sequences since they variable length sequences can cause problems in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9975e18-118d-40f9-969c-b5b8fb10d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels, dtype=\"float32\")\n",
    "val_labels = np.array(val_labels, dtype=\"float32\")\n",
    "\n",
    "# making sure the labels have the correct dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "809b1e9f-b3e5-444d-9a03-ccf0ed339ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to tf datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sequences, train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_sequences, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a81c3576-68f6-45c7-95a5-20d007746229",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = train_dataset.shuffle(len(train_sequences)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8265ae63-5374-4190-953d-54dbfe3fa2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ masking (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)                    │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ gaussian_noise (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GaussianNoise</span>)       │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ masking (\u001b[38;5;33mMasking\u001b[0m)                    │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ gaussian_noise (\u001b[38;5;33mGaussianNoise\u001b[0m)       │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Masking(mask_value= -999.0), # masking the padded parts of the sequence\n",
    "    tf.keras.layers.GaussianNoise(stddev=0.01), # for robustness\n",
    "    tf.keras.layers.LSTM(128, return_sequences=False, dropout=0.1, recurrent_dropout=0.05, use_bias=False),  \n",
    "    tf.keras.layers.Dense(train_labels.shape[1], use_bias=False) # no bias to force the model to not learn averages but use past values for predictions\n",
    "]) # i will optimize the model architecture in further iterations. so far, it seems like larger and deeper networks lead to overfitting.\n",
    "\n",
    "# since the data by its nature is very noisy we need to be vary of overfitting to noise! \n",
    "\n",
    "model.compile(optimizer=\"Adam\", loss=\"MSE\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ede147-2992-4efd-bbb9-81ac64fa0cc3",
   "metadata": {},
   "source": [
    "With LSTMs, adding more layers or more neurons seems to lead to overfitting issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ccbc23a-4ea7-4501-b227-a2acf025b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "# early stopping so we stop training when the model stops overfitting, and only save the best model based on out of sample estimates.\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.8, patience=1, min_lr=1e-6)\n",
    "# reduce_lr helps the model stop diverging if its learning rate becomes too large as it gets closer to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c5cb286-7ad2-47a2-9aad-93654cfd8929",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 0.4660 - val_loss: 0.3177 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.3163 - val_loss: 0.3076 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.3085 - val_loss: 0.3060 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.3016 - val_loss: 0.2998 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.2983 - val_loss: 0.3008 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2923 - val_loss: 0.2989 - learning_rate: 8.0000e-04\n",
      "Epoch 7/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.2923 - val_loss: 0.2947 - learning_rate: 8.0000e-04\n",
      "Epoch 8/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.2901 - val_loss: 0.2963 - learning_rate: 8.0000e-04\n",
      "Epoch 9/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.2868 - val_loss: 0.2938 - learning_rate: 6.4000e-04\n",
      "Epoch 10/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2913 - val_loss: 0.2940 - learning_rate: 6.4000e-04\n",
      "Epoch 11/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.2885 - val_loss: 0.2933 - learning_rate: 5.1200e-04\n",
      "Epoch 12/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2839 - val_loss: 0.2934 - learning_rate: 5.1200e-04\n",
      "Epoch 13/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2910 - val_loss: 0.2940 - learning_rate: 4.0960e-04\n",
      "Epoch 14/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.2893 - val_loss: 0.2928 - learning_rate: 3.2768e-04\n",
      "Epoch 15/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2863 - val_loss: 0.2917 - learning_rate: 3.2768e-04\n",
      "Epoch 16/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2866 - val_loss: 0.2915 - learning_rate: 3.2768e-04\n",
      "Epoch 17/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.2854 - val_loss: 0.2921 - learning_rate: 3.2768e-04\n",
      "Epoch 18/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2824 - val_loss: 0.2916 - learning_rate: 2.6214e-04\n",
      "Epoch 19/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2810 - val_loss: 0.2916 - learning_rate: 2.0972e-04\n",
      "Epoch 20/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2871 - val_loss: 0.2918 - learning_rate: 1.6777e-04\n",
      "Epoch 21/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2853 - val_loss: 0.2911 - learning_rate: 1.3422e-04\n",
      "Epoch 22/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2829 - val_loss: 0.2914 - learning_rate: 1.3422e-04\n",
      "Epoch 23/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.2800 - val_loss: 0.2910 - learning_rate: 1.0737e-04\n",
      "Epoch 24/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2817 - val_loss: 0.2912 - learning_rate: 8.5899e-05\n",
      "Epoch 25/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2820 - val_loss: 0.2908 - learning_rate: 6.8719e-05\n",
      "Epoch 26/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2803 - val_loss: 0.2915 - learning_rate: 6.8719e-05\n",
      "Epoch 27/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2816 - val_loss: 0.2908 - learning_rate: 5.4976e-05\n",
      "Epoch 28/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2793 - val_loss: 0.2910 - learning_rate: 4.3980e-05\n",
      "Epoch 29/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2823 - val_loss: 0.2908 - learning_rate: 3.5184e-05\n",
      "Epoch 30/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2794 - val_loss: 0.2910 - learning_rate: 2.8147e-05\n",
      "Epoch 31/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2772 - val_loss: 0.2911 - learning_rate: 2.2518e-05\n",
      "Epoch 32/1000\n",
      "\u001b[1m445/445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2804 - val_loss: 0.2909 - learning_rate: 1.8014e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=1000, batch_size=32, \n",
    "                    validation_data=val_dataset, callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ae026-e396-4da0-b829-8676c8ee77dc",
   "metadata": {},
   "source": [
    "The MSE loss by itself doesn't mean much. Let's see some actual predictions to see how close we're getting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8516ce2-bccf-49f6-a4cd-69b8f9d3190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_columns = data.drop(columns=[\"PLAYER_NAME\", \"SEASON_START_YEAR\", \"PLAYER_AGE\", \"is_traded\"]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10f7da55-5678-4b02-9cb8-c965c8d0b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_normalize(normalized_preds, means, stds, columns):\n",
    "    \"\"\"\n",
    "    Retransform normalized predictions to the original scale.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(normalized_preds.shape) == 1:\n",
    "        original_preds = np.empty_like(normalized_preds)\n",
    "        for i, col in enumerate(columns):\n",
    "            original_preds[i] = normalized_preds[i] * stds[col] + means[col]\n",
    "    else:\n",
    "        original_preds = np.empty_like(normalized_preds)\n",
    "        for i, col in enumerate(columns):\n",
    "            original_preds[:, i] = normalized_preds[:, i] * stds[col] + means[col]\n",
    "    \n",
    "    return original_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742bea0b-75af-4fdf-8dd9-4cf0b99d6799",
   "metadata": {},
   "source": [
    "Let's predict a random historical season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6735855a-c582-4371-a3c9-6ed9c124515f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions vs Actual Values:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step\n",
      "Player: Larry Kenon, Year: 1983\n",
      "Stats:\n",
      "  GP: Actual = 48.0, Prediction = 58.0\n",
      "  MIN_PER_GAME: Actual = 16.0, Prediction = 17.9\n",
      "  FGM_PER_GAME: Actual = 2.5, Prediction = 3.1\n",
      "  FGA_PER_GAME: Actual = 5.3, Prediction = 6.4\n",
      "  FG3M_PER_GAME: Actual = -0.0, Prediction = 0.0\n",
      "  FG3A_PER_GAME: Actual = 0.0, Prediction = 0.0\n",
      "  FTM_PER_GAME: Actual = 0.9, Prediction = 1.1\n",
      "  FTA_PER_GAME: Actual = 1.2, Prediction = 1.6\n",
      "  OREB_PER_GAME: Actual = 1.4, Prediction = 1.2\n",
      "  DREB_PER_GAME: Actual = 1.7, Prediction = 2.0\n",
      "  REB_PER_GAME: Actual = 3.1, Prediction = 3.2\n",
      "  AST_PER_GAME: Actual = 0.8, Prediction = 1.3\n",
      "  STL_PER_GAME: Actual = 0.5, Prediction = 0.6\n",
      "  BLK_PER_GAME: Actual = 0.2, Prediction = 0.1\n",
      "  TOV_PER_GAME: Actual = 1.0, Prediction = 1.3\n",
      "  PF_PER_GAME: Actual = 1.3, Prediction = 1.6\n",
      "  PTS_PER_GAME: Actual = 5.8, Prediction = 7.2\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPredictions vs Actual Values:\")\n",
    "\n",
    "num_predictions = 1\n",
    "random_indices = random.sample(range(len(val_sequences)), num_predictions)\n",
    "\n",
    "for idx in random_indices:\n",
    "    seq = val_sequences[idx:idx+1]  # select the sequence\n",
    "    actual = val_labels[idx]         # select the actual label\n",
    "    prediction = model.predict(seq)  # make prediction\n",
    "    # retransform the normalized prediction back to the original scale\n",
    "    original_prediction = inverse_normalize(prediction, means, stds, stats_columns)\n",
    "    original_actual = inverse_normalize(actual, means, stds, stats_columns)\n",
    "    \n",
    "    player_name, season_year = val_player_info[idx]  # get player info\n",
    "    print(f\"Player: {player_name}, Year: {season_year * stds[\"SEASON_START_YEAR\"] + means[\"SEASON_START_YEAR\"]:.0f}\")\n",
    "    print(\"Stats:\")\n",
    "    for stat, actual_val, pred_val in zip(stats_columns, original_actual, original_prediction[0]):\n",
    "        print(f\"  {stat}: Actual = {actual_val:.1f}, Prediction = {pred_val:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef7f91b-d694-4cb2-b9bf-e6f79a3275e3",
   "metadata": {},
   "source": [
    "Finally, let's save the model and parameters to reuse them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47e44206-efc8-45cc-a162-92aba4c614a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model for reuse\n",
    "\n",
    "model.save(\"nba_stats_predictor_model.keras\")\n",
    "\n",
    "normalization_params = {\n",
    "    \"means\": means,\n",
    "    \"stds\": stds,\n",
    "    \"stats_columns\": stats_columns\n",
    "}\n",
    "\n",
    "with open(\"normalization_params.json\", \"w\") as f:\n",
    "    means_dict = {k: float(v) for k, v in means.items()}\n",
    "    stds_dict = {k: float(v) for k, v in stds.items()}\n",
    "    params = {\n",
    "        \"means\": means_dict,\n",
    "        \"stds\": stds_dict,\n",
    "        \"stats_columns\": stats_columns\n",
    "    }\n",
    "    json.dump(params, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
